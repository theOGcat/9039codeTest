{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as gb\n",
    "import os\n",
    "import keras_tuner as KT\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from sklearn.utils import resample\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import copy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "# Enable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('archive/train.csv',sep=\" \",header=None)\n",
    "train_df.columns=['patient id', 'filename', 'label','data source']\n",
    "train_df=train_df.drop(['patient id', 'data source'], axis=1 )\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('archive/test.csv',sep=\" \",header=None)\n",
    "test_df.columns=['patient id', 'filename', 'label','data source']\n",
    "test_df=test_df.drop(['patient id', 'data source'], axis=1 )\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_count = train_df['label'].value_counts()['positive']\n",
    "train_n_count = train_df['label'].value_counts()['negative']\n",
    "\n",
    "print(train_df['label'].value_counts())\n",
    "print(test_df['label'].value_counts())\n",
    "train_df = shuffle(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_path = 'archive/train'\n",
    "test_folder_path = 'archive/test'\n",
    "\n",
    "#Print out each folder's info\n",
    "\n",
    "def count_photos(folder_path):\n",
    "    count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_photos_in_folders(train_folder_path, test_folder_path):\n",
    "    train_count = count_photos(train_folder_path)\n",
    "    test_count = count_photos(test_folder_path)\n",
    "    return (train_count, test_count)\n",
    "\n",
    "# 将数据信息打印出来\n",
    "# 2. Print out info for each dataset\n",
    "train_count, test_count = count_photos_in_folders(train_folder_path, test_folder_path)\n",
    "\n",
    "print(f\"Train folder contains {train_count} photos.\")\n",
    "print(f\"Test folder contains {test_count} photos.\")\n",
    "\n",
    "train_data_size = [train_p_count, train_n_count]\n",
    "labels = ['positive', 'negative']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12, 8))\n",
    "sns.barplot(x=train_data_size, y=labels, ax=axs)\n",
    "axs.set_title('Training data categories')\n",
    "axs.set(xlabel='Image number')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative  = train_df[train_df['label']=='negative']   #negative values in class column\n",
    "positive = train_df[train_df['label']=='positive']  #positive values in class column\n",
    "\n",
    "df_majority_downsampled_neg = resample(negative, replace = True, n_samples = 5000)\n",
    "df_majority_downsampled_pos = resample(positive, replace = True, n_samples = 5000)\n",
    "\n",
    "train_df = pd.concat([df_majority_downsampled_pos, df_majority_downsampled_neg])\n",
    "train_df = shuffle(train_df) # shuffling so that there is particular sequence\n",
    "\n",
    "train_df, valid_df = train_test_split(train_df, train_size=0.8, random_state=0)\n",
    "\n",
    "print(f\"Negative and positive values of train:\\n {train_df['label'].value_counts()}\")\n",
    "print(f\"Negative and positive values of validation:\\n {valid_df['label'].value_counts()}\")\n",
    "print(f\"Negative and positive values of test:\\n {test_df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，把train分成train set和validation set，将batch size设置为64\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                   shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, vertical_flip =True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "#Now fit the them to get the images from directory (name of the images are given in dataframe) with augmentation\n",
    "\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(dataframe = train_df, directory='archive/train', x_col='filename', \n",
    "                                              y_col='label', target_size=(256,256), batch_size=64, \n",
    "                                               class_mode='binary')\n",
    "val_gen = test_datagen.flow_from_dataframe(dataframe = valid_df, directory='archive/train', x_col='filename',\n",
    "                                             y_col='label', target_size=(256,256), batch_size=64, \n",
    "                                            class_mode='binary')\n",
    "test_gen = test_datagen.flow_from_dataframe(dataframe = test_df, directory='archive/test', x_col='filename', \n",
    "                                            y_col='label', target_size=(256,256), batch_size=64,\n",
    "                                             class_mode='binary')\n",
    "labels = {value: key for key, value in train_gen.class_indices.items()}\n",
    "#class mode binary because we want the classifier to predict covid or not\n",
    "#target size (200,200) means we want the images to resized to 200*200\n",
    "\n",
    "#Examine the first image in the training dataset and print the label which corresponding to it.\n",
    "print(labels)\n",
    "images, labels = next(train_gen)\n",
    "# Get the filenames associated with the images in the batch\n",
    "filenames = train_gen.filenames\n",
    "# Print the filename and label for the first image in the batch\n",
    "print(filenames[0], labels[0])\n",
    "# Plot the first image in the batch\n",
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is used for tuning the CNN section, we will compare the CNN before and after tuning.\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    # Add layers\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=hp.Choice('Activation Function', ['relu', 'tanh']), input_shape=(256, 256, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Dropout rate was added to prevent overfitting.\n",
    "        layers.Conv2D(64, (3, 3), activation=hp.Choice('Activation Function', ['relu', 'tanh'])),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation=hp.Choice('Activation Function', ['relu', 'tanh'])),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(hp.Choice('Drop out Rate', [0.5, 0.25])),\n",
    "\n",
    "        layers.Conv2D(256, (3, 3), activation=hp.Choice('Activation Function', ['relu', 'tanh'])),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(hp.Choice('Drop out Rate', [0.5, 0.25])),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation=hp.Choice('Activation Function', ['relu', 'tanh'])),\n",
    "        layers.Dense(1, activation=hp.Choice('Output Activation Function', ['softmax', 'sigmoid']))\n",
    "    ])\n",
    "\n",
    "    opt = Adam(learning_rate=hp.Choice(\n",
    "        'Learning Rate', values=[1e-3, 1e-4, 1e-5, 1e-6]))\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    # historyTuning = CNNmodel.fit(train_gen,\n",
    "    #                 epochs=30,\n",
    "    #                 validation_data=val_gen,\n",
    "    #                 callbacks=[early_stop])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 使用tuner来找出最佳的超参数组合，最佳的标准是拥有最好的validation accuracy，然后最多排列尝试次数是100次\n",
    "# 4.Select best model\n",
    "tuner = KT.RandomSearch(hypermodel=build_model, objective=\"val_accuracy\", max_trials=20,\n",
    "                        directory='./tuner', overwrite=True, project_name='Project_CNN_tuner')\n",
    "tuner.search_space_summary()\n",
    "# tuning过程的epoch设置成2（也就是每一种超参数的组合都run两次epoch），设置太大的话tuning过程会及其长\n",
    "tuner.search(train_gen, epochs=2, validation_data=val_gen)\n",
    "# 将validation accuracy最高的模型储存为best\n",
    "best_model = tuner.get_best_models()[0]\n",
    "# best model的summary\n",
    "best_model.summary()\n",
    "# 把模型储存在本地，这个模型是只加载CNN的情况下做好的tuning.\n",
    "best_model.save('Best_CNN_Only')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###### 做epoch30的统计图，选一个最佳 ######\n",
    "# 加载储存了fine-tuning CNN的模型\n",
    "best_model_only = keras.models.load_model('Best_CNN_only')\n",
    "# 储存epoch 30的history信息\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = best_model_only.fit(train_gen,\n",
    "                    epochs=10,\n",
    "                    validation_data=val_gen,\n",
    "                    callbacks=[early_stop])\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = best_model_only.evaluate(test_gen, steps=test_gen.samples // test_gen.batch_size)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(train_loss)\n",
    "print(train_accuracy)\n",
    "print(val_loss)\n",
    "print(val_accuracy)\n",
    "# 作两张图，training loss vs validation loss和training accuracy vs validation accuracy\n",
    "# 以此判断在epoch大于多少时模型开始overfitting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(12, 10))\n",
    "ax[0].set_title('Loss vs Epoch')\n",
    "ax[0].plot(history.history['loss'], label='Training Loss', color='orange')\n",
    "ax[0].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend(loc='best')\n",
    "ax[1].set_title('Accuracy vs Epoch')\n",
    "ax[1].plot(history.history['accuracy'], label='Training Accuracy', color='orange')\n",
    "ax[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
